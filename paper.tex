\documentclass{sig-alternate}
\usepackage{color}

\newcommand{\red}[1]{\textcolor{red}{#1}}

\title{Adaptation Writ Large:\\ An Essay in Memory of John H. Holland}

\numberofauthors{2}
\author 
{\alignauthor
 Stephanie Forrest\\
 \affaddr{University of New Mexico}\\
 \affaddr{Santa Fe Institute}\\
 \email{forrest@cs.unm.edu}
 \alignauthor
Melanie Mitchell \\
 \affaddr{Portland State University}\\
 \affaddr{Santa Fe Institute}\\
 \email{mm@pdx.edu}
}

\begin{document}
\maketitle

% \begin{abstract}
% \input{abstract}
% \end{abstract}

\section{Introduction}

Professor John H. Holland passed away recently in Ann Arbor, MI, where
he had been on the University of Michigan faculty for over 50 years.
John, as he was known universally to his colleagues and students,
leaves behind a long legacy of intellectual achievements.  

John was a direct intellectual descendant of the cybernetics era, and
early on was strongly influenced by the work of von Neumann, Wiener,
Ashby, and Turing, all of whom viewed computation as a broad,
interdisciplinary enterprise.  Thus, John became an early proponent of
interdisciplinary approaches to computer science and was an active
evangelist of what is now called {\emph computational thinking}, reaching out
enthusiastically to psychologists, economists, physicists, linguists,
philosophers, and pretty much anyone he came in contact with.  As a
result, even though he received what was arguably the world's first
computer science Ph.D. in 1959, his contributions are sometimes better known
outside of CS than within.

John Holland is best known for his invention of {\emph genetic
  algorithms} (GAs), a family of search and learning methods inspired
by biological evolution.  Since their invention in the 1960s, GAs,
along with related evolutionary computation methods, have become a
thriving subfield of computer science, with widespread scientific and
commercial applications.  While the mechanisms of GAs are well-known
to much of the CS community, fewer are aware that GAs were only one
offshoot of Holland's much broader motivation---to develop a general
theory of adaptation in complex systems.  This motivation was the
driving force in all of Holland's research.   

In this short essay, we sketch four key, recurrent themes of Holland's
work on adaptive systems: (1) the evolutionary dynamics of ``building
blocks''; (2) learning via credit assignment and rule discovery; (3)
the emergence of internal models; and (4) the generic properties of
complex adaptive systems.  We discuss the role these themes have
played in computer science, and highlight especially the ideas that we
think remain relevant to today's research agendas.

\section{Evolutionary Dynamics of ``Building Blocks''}  

Holland's goal of developing a general theory of adaptation was
spurred both by his early work on computer models of Hebbian learning
\cite{RochesterHolland} and his reading of Ronald Fisher's classic
work on integrating genetics with Darwinian selection \cite{Genetical
  Theory of Natural Selection}.  As Holland further read extensively
in evolutionary biology, economics, game theory, and control theory,
he came to recognize that adaptation was the most central concept in
all these fields.  That is, these fields all concern populations of
agents that must continually obtain information from uncertain
changing environments and use it in order to improve their performance
with respect to those environments.  Moreover, Holland recognized that
in systems with {\em adaptive agents} of this kind, there is never a
state of {\emph equilibrium} or a final {\emph optimum} configuration.
Due to the environment's ``perpetual novelty'' (as Holland termed it),
adaptation continues forever in an open-ended way.  Holland's focus on
open-ended, non-equilibrium dynamics was in stark contrast with the
mainstream approach (at the time) in all these fields---the belief
that ``solving for'' stable equilibrium dynamics was the scientific
goal.  Holland's contrary view was that a system in stable equilibrium
is essentially {\emph dead}.

Underlying Holland's theory of adaptation are three core ideas: 

\begin{itemize}
\item {\bf Populations:} Adaptation occurs in populations of
  individuals evolving (or learning) over time, in which statistics 
  can be leveraged to direct population dynamics.  (More on this below.)  

\item {\bf Building Blocks:} In a population undergoing adaptation,
  individuals can be decomposed into {\emph building blocks}---sets of
  traits that are the evolutionary ``atoms'' of an individual's
  fitness or performance. As an example from biology, Holland gives
  the {\emph Krebs cycle}, a core cellular metabolic
  pathway. Similarly, in neuroscience, a building block of visual
  perception might be a group of neurons that responds to an oriented
  edge; in computer science, a building block of a sorting program
  might be a ``swap'' operator.

\item {\bf Exploitation Versus Exploration:} Successful adaptation
  requires the right tradeoff between {\emph exploitation}, in which
  tried-and-true building blocks propagate in a population, and {\emph
    exploration}, in which existing building blocks are recombined or
  mutated in new ways.  A system that focuses too much on exploitation
  risks never finding better individuals or not being able to adapt to
  changing environments.  Conversely, too much focus on exploration
  means that the system won't leverage the successful building blocks
  that have already been discovered.  There is some optimal balance
  between these two modes of processing.

\end{itemize}

Several of Holland's early papers (e.g.,
\cite{OutlineLogicalTheory,OptimalAllocation}) and his influential
1975 book {\emph Adaptation in Natural and Artificial Systems}
\cite{ANAS} developed a general, formal setting in which these ideas
could be expressed mathematically.

In particular, inspired by Bellman \cite{AdaptiveControlProcesses} and
others, Holland explored the exploitation-versus-exploration tradeoff
via an idealized ``two-armed bandit'' problem.  Given a slot machine
with two arms, each of which has an unknown payoff probability, how
should you allocate $N$ trials (pulls) between the arms so as to
maximize your total payoff?  An example of an extreme {\emph
  exploitation} strategy would be to alternate between the arms until
one of them gives a payoff, and then allocate all future trials to
that arm alone.  Conversely, an example of an extreme {\emph
  exploration} strategy would be to randomly allocate the trials,
irrespective of the payoff rates you obtain.  Obviously, each of these
strategies is flawed.  What is an optimal strategy?

In \cite{OptimalAllocationOfTrials} and \cite{ANAS}, Holland derived
an equation for such a strategy.  Let $A$ denote the arm currently
observed to have the higher payoff probability, and $B$ denote the
other arm.  Holland showed that the optimal strategy (the one that
yields maximal payoff, or minimal loss) is for the number of trials
allocated to $A$ to grow slightly faster than an exponential function
of the number of trials allocated to $B$.  Holland then showed that this
result extends naturally to the {\emph multi-armed bandit} case.

In an population undergoing adaptation, the building blocks making up
individuals can be compared to the arms of a multi-armed bandit.  Each
building block can be said to have a probability of ``payoff'' (i.e.,
contribution to a given individual's fitness).  Evaluating an
individual in an environment is like ``pulling the arms'' on a
multi-armed bandit, where the ``arms'' correspond to each of the
building blocks making up that individual.  Thus, in the process of
assigning a fitness to each individual in a population, adaptive
evolution can be seen as implicitly sampling the many building blocks
making up that collection of individuals.  The average fitness over
many individuals containing a particular building block gives an
estimate of that building block's payoff probability.

The question of how to balance exploitation and exploration---how to
optimally allocate trials to different arms based on their observed
payoff---now becomes the question of how to optimally sample in the
vast space of possible building blocks, based on their observed
average fitness.  Of course adaptive evolution deals in populations of
individuals, not building blocks.  There is no explicit mechanism for
keeping statistics on building blocks' ``observed average fitness''.
However, Holland's central idea here is that an approximation to
optimal building-block sampling does indeed occur, as an emergent
property of the population dynamics.

To show this mathematically, Holland defined an idealization of
adaptive evolution: an algorithm he called a {\emph reproductive
  plan}.  A reproductive plan is a population-based stochastic process
operating on bit strings, involving the ``genetic'' operators of
reproduction, fitness-based selection, crossover, mutation, and
inversion.  Building blocks are formalized as {\emph
  schemata}---patterns of bits within a string. (Here are two examples
of schemas: ``all strings beginning with the bits 10'' or ``all
strings that start with the pattern 1*1'', where * can be replaced by
either 0 or 1).

Holland's most important result was the following: He proved
mathematically that his reproductive plan results---implicitly---in a
near-optimal allocation of trials to schemata, thus optimizing the
exploitation versus exploration balance.  Holland noted that the
explicit act of assigning fitnesses to the individuals in a population
was actually implicitly sampling a much larger collection of building
blocks.  Holland termed this {\emph implicit parallelism}; the
parallel leveraging of statistics from implicit sampling is a main
strength of his population-based approaches to search.

It was this formal setting and resulting theorems that led to the
invention of genetic algorithms, which featured stochastic
population-based search and {\emph crossover} as a critical
operation: Crossover recombined successful building blocks and
thus allowed them to be tested in new contexts.  What made GAs unique
among other evolution-inspired algorithms at the time were the
mathematical foundations described above, including bit-string
representations, the focus on recombination as a central mechanism,
and a focus (at least in Holland's mind) on continual adaptation to
non-stationary environments rather than optimization to static
environments.

It is important to point out again that the framework Holland
developed in \cite{ANAS} was more general than the genetic algorithm;
it was created in order to develop an interdisciplinary theory of
adaptation, one that would inform biology, say, as much as computer
science \cite{ChristiansenFeldmanPaper}.  The later, successful
application of genetic algorithms to real-world optimization and
learning tasks was, for Holland, just icing on the cake.

\section{Learning Via Credit Assignment and Rule Discovery}

\section{Internal Models}

The concept of {\emph internal models} is central to Holland's theory
of adaptive systems.  He posits that all adaptive systems create and
use internal models to survive in their environments, through
anticipation.  Models can be tacit and learned through evolutionary
time, as in the case of bacteria swimming up a chemical gradient, or
explicit and learned over a single lifespan as in the case of
cognitive systems that incorporate experience into internal rule sets
through learning.  In \underline{Induction}, Holland and his
co-authors discuss how such internal models can be learned, without
supervision, by combining environmental inputs with stored knowledge.
A key idea is that a model defines an equivalence relation over a set
of environmental states, together with a set of transition rules,
which are learned over time based on environmental inputs.  Models
that form valid homomorphisms with the environment allow the system to
make accurate predictions.  In his conception, the equivalence classes
are initially very general, say 'moving object' and 'stationary
object,' and over time through experience and learning they are
specialized into more useful and precise subclasses, say 'insect' and
'nest', which form a default hierarchy and are associated with more
specific transition rules.  Although the idea of default hierarchies
is prevalent in many knowledge representation systems (LIST A FEW?),
Holland made two key contributions, first by emphsizing homomorphisms
as a formal way to evaluate model validity, and idea that dates back
to Ross Ashby's \underline{An Introduction to Cybernetics} that
Holland's student, Bernard Ziegler, developed into a formal theory of
computer modeling and simulation.  Today, even though computational
modeling is a major application area of computing, essential to most
natural sciences and engineering disciplines, these early homorphic
theories of modeling stand as the most elegant approach we know of for
characterizing when a model is consistent with the environment and how
an intelligent agent, human or artificial, can update the model to
better reflect reality.

Holland's second key contribution was describing a computational
mechanism by which a cognitive system could iteratively build up a
detailed and hierarchical model of its environment, as Melanie already
discussed above.

Given that Holland believed that the ability to learn and manipulate
internal models was essential for any adapting system, it is no
suprise that he viewed modeling as essential for scientific inquiry.
Today, we use computational models in several distinct ways---as a
tool for analyzing data in statistical models, through simulation as a
method for discovering new knowledge, and as a way to explain or
understand how a system works.  This third use of models was dear to
Holland's heart.  In his view, the key to science was understanding
the mechanisms that cause a system to behave in a certain way, an
aspiration that goes well beyond data fitting methods, which typically
focus only on describing the aggregate behavior of a system.  For
example, a statistical model that describes the boom and bust pattern
of the stock market cannot address the underlying mechanisms that lead
to these cycles, through the collective actions of many many
individual buy/sell decisions.  Similarly, the genetic algorithms for
which he is so famous provide a simple computational framework in
which to explore the dynamics of Darwinian evolution and whether the
basic mechanisms of variation, differential reproduction, and heredity
are sufficient to account for the richness of our natural world.  This
emphasis on exploratory models to build intuitions was an important
theme of Holland's work, and he often quoted Eddington's remark on the
occasion of the first experimental test of Einstein's theory of
relativity: "The contemplation in natural science of a wider domain
than the actual leads to a far better understanding of the actual."
Holland was interested in models that explored basic principles and
mechanisms, even if they didn't make specific predcitions, models that
could show generically how certain behaviors could be produced, for
example how exponential growth might interact with finite resources.
Holland pioneered a style of modeling that has come to be known as
'individual based' or 'agent based,' in which every component of a
system is represented explicitly and has state, e.g., every trader in
a stock market system or every cell in an immune system model, and
each agent had its own behaviors, which it could update (learn) over
time.  These models are often defined over spatial structures, such as
networks or simple grids, to capture the constraints of systems living
under spatial constraints.  An agent-based model, then, encodes a
theory about the mechanisms that are relevant for producting the
behavior of interest.  Similar to expert systems, such models are
especially useful for studying systems that don't have analytic
mathematical descriptions and they can facilitate interdisicplinary
collaborations because the underlying rules can be easily
communicated.  It should be noted that Holland's view of modeling is
by no means typical.  For example, in a textbook on computational
modeling, the authors offer the following definition: “Modeling is the
application of methods to analyze complex, real-world problems in
order to make predictions about what might happen with various
actions.” [Shiflet and Shiflet, 2006].  This sort of perspective
completely rules out the kind of modeling that Holland was most
interested in, namely exploratory modeling.  Similarly, many dispute
that models make any kind of scientific contribution: “Models are
metaphors that explain the world we don’t understand in terms of
worlds we do.  They are merely analogies, provide partial insight,
stand on someone else’s feet.  Theories stand on their own feet, and
rely on no analogies.”  [Emanuel Derman, 2012].  [NOW WE NEED A STRONG
  FINISHING SENTENCE TO RESCUE JHH STYLE MODELING.]

\section{Complexity}

\section{Relevance to Modern CS}

\begin{itemize}
\item Exploitation versus exploration -- relevance to reinforcement
  learning, other parts of machine learning and optimization.
\item Active learning.  (Two-armed bandit problem.) 
\item On-line learning.  
\end{itemize}

\section{Conclusion}

Introduced a few generations of students to computation in natural systems, an idea that today is better accepted.  His insights were deeper and more general than what often passes for work in biomimicry, e.g., for robots.

The ideas have had huge impact and should still be a beacon for research in intelligent and complex systems

John's personality and humanity is inextricably tangled up with his intellectual contributions.


\end{document}
