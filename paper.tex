\documentclass{sig-alternate}
\usepackage{color}

\newcommand{\red}[1]{\textcolor{red}{#1}}

%\title{Adaptation Writ Large:\\ An Essay in Memory of JohnH. Holland}
\title{Adaptative Computation:\\ An Essay in Memory of John H. Holland}

\numberofauthors{2}
\author 
{\alignauthor
 Stephanie Forrest\\
 \affaddr{University of New Mexico}\\
 \affaddr{Santa Fe Institute}\\
 \email{forrest@cs.unm.edu}
 \alignauthor
Melanie Mitchell \\
 \affaddr{Portland State University}\\
 \affaddr{Santa Fe Institute}\\
 \email{mm@pdx.edu}
}

\begin{document}
\maketitle

% \begin{abstract}
% \input{abstract}
% \end{abstract}

\section{Introduction}

Professor John H. Holland passed away recently in Ann Arbor, MI, where
he had been on the University of Michigan faculty for over 50 years.
John, as he was known universally to his colleagues and students,
leaves behind a long legacy of intellectual achievements.  

John was a direct intellectual descendant of the cybernetics era, and
early on was strongly influenced by the work of von Neumann, Wiener,
Ashby, and Turing, all of whom viewed computation as a broad,
interdisciplinary enterprise.  Thus, John became an early proponent of
interdisciplinary approaches to computer science and was an active
evangelist of what is now called {\emph computational thinking}, reaching out
enthusiastically to psychologists, economists, physicists, linguists,
philosophers, and pretty much anyone he came in contact with.  As a
result, even though he received what was arguably the world's first
computer science Ph.D. in 1959, his contributions are sometimes better known
outside of CS than within.

John Holland is best known for his invention of {\emph genetic
  algorithms} (GAs), a family of search and learning methods inspired
by biological evolution.  Since their invention in the 1960s, GAs,
along with related evolutionary computation methods, have become a
thriving subfield of computer science, with widespread scientific and
commercial applications.  While the mechanisms of GAs are well-known
to much of the CS community, fewer are aware that GAs were only one
offshoot of Holland's much broader motivation---to develop a general
theory of adaptation in complex systems.  This motivation was the
driving force in all of Holland's research.   

In this short essay, we sketch five key, recurrent themes of Holland's
work on adaptive systems: (1) the evolutionary dynamics of ``building
blocks''; (2) learning via credit assignment and rule discovery; (3)
the emergence of internal models; (4) the art of scientific modeling;
and (5) the universal properties of complex adaptive systems.  We
discuss the role these themes have played in computer science, and
highlight especially the ideas that we think remain relevant to
today's research agendas.

\section{Evolutionary Dynamics of ``Building Blocks''}  

Holland's goal of developing a general theory of adaptation was
spurred both by his early work on computer models of Hebbian learning
\cite{Rochester1956} and his reading of Ronald Fisher's classic work
on integrating genetics with Darwinian selection \cite{Fisher1930}.
As Holland further read extensively in evolutionary biology,
economics, game theory, and control theory, he came to recognize that
adaptation was the most central concept in all these fields.  That is,
these fields all concern populations of agents that must continually
obtain information from uncertain changing environments and use it in
order to improve their performance with respect to those environments.
Moreover, Holland recognized that in systems with {\em adaptive
  agents} of this kind, there is never a state of {\emph equilibrium}
or a final \emph{optimum} configuration.  Due to the environment's
perpetual novelty,
%SF thinks Holland did not invent this term (as Holland termed it), 
adaptation is a continual 
open-ended process.  Holland's focus on open-ended,
non-equilibrium dynamics was in stark contrast with the mainstream
approach (at the time) in all these fields---the belief that ``solving
for'' stable equilibrium dynamics was the scientific goal.  Holland's
contrary view was that a system in stable equilibrium is essentially
\emph{dead}.

% Text that STEPH found in our NIPS paper on the IGA
% First, at least theoretically the GA is fast because of implicit
% parallelism (Holland, 1975/1992): each string in the population is an
% instance of many different schemas, and if the population is large
% enough and is initially chosen at random, a large number of different
% schemas-many more than the number of strings in the population-are
% being sampled in parallel.  This should result in a quick search for
% short, low-order schemas that confer high fitness.  Second,
% fitness-proportionate reproduction under the GA should conserve
% instances of such schemas.  Third, a high crossover rate should
% quickly combine instances oflow-order schemas on different strings to
% create instances of longer schemas that confer even higher fitness.

Underlying Holland's theory of adaptation are three core ideas:
Population-based search, building blocks and recombination, and the
exploration-exploitation tradeoff.

\begin{itemize}
\item{\bf Populations and implicit parallelism:} A population of
  individuals can be thought of as a set of samples taken from a much larger
  search space.  Importantly, the populations are competitive, and
  the evolutionary dynamic biases the set of samples
  over time towards high-fitness regions of the search space. 

%The population can be
%  thought of as a set of samples in a much larger search space, where
%  the processes of evolution
%  population can be thought of as a set of sa in which statistics 
%  can be leveraged to direct population dynamics.  (More on this below.)  

\item{\bf Building Blocks and recombination:} High-fitness individuals
  are discovered in stages, first by finding high-fitness partial
  solutions (the building blocks) through random sampling, and over
  time assembling the partial solutions into more complete solutions
  with even higher fitness.  For example, when searching the space of
  all $n$-bit binary strings, almost any random sample would contain
  examples of strings whose first bit is set to 1, and also many
  samples of strings whose second bit is set to 1, but there would be
  many fewer samples that have both of the first two bits set to
  1.  More compelling examples include the \emph{Krebs cycle}, a core
  cellular metabolic pathway, a group of neurons that responds to an
  oriented edge in vision, or the ``swap'' operator in a sorting
  algorithm.  

 To succeed, such a search requires first that
  high-fitness subparts be preserved once found, and second, that
  there be an efficient mechanism for combining promising building
  blocks into higher-order building blocks.  Holland proposed using an
  analog of Darwinian selection for the first and the idea of crossing
  over in genetics for the second.  Holland referred to the building
  blocks as \emph{schemas,} which correspond mathematically to hyperplanes in the
  search space.


%In a population undergoing adaptation,
%  individuals can be decomposed into {\emph building blocks}---sets of
% traits that are the evolutionary ``atoms'' of an individual's
%  fitness or performance. 

\item{\bf Exploitation Versus Exploration:} Successful adaptation
  requires maintaining a balance between {\emph exploitation}, in which
  tried-and-true building blocks propagate in a population, and {\emph
    exploration}, in which existing building blocks are recombined or
  mutated in new ways.  
%A system that focuses too much on exploitation
%  risks never finding better individuals or not being able to adapt to
%  changing environments.  Conversely, too much focus on exploration
%  means that the system won't leverage the successful building blocks
%  that have already been discovered.  %There is some optimal balance
%  between these two modes of processing.
\end{itemize}

Several of Holland's early papers (e.g.,
\cite{Holland1962,Holland1973}) and his influential
1975 book {\emph Adaptation in Natural and Artificial Systems}
\cite{Holland1975} developed a general, formal setting in which these ideas
could be expressed mathematically.

Inspired by Bellman \cite{Bellman1961} and
others, Holland explored the exploitation-versus-exploration tradeoff
via an idealized ``two-armed bandit'' problem.  Given a slot machine
with two arms, each of which has an unknown payoff probability, how
should you allocate $N$ trials (pulls) between the arms so as to
maximize your total payoff?  An example of an extreme {\emph
  exploitation} strategy would be to alternate between the arms until
one of them gives a payoff, and then allocate all future trials to
that arm alone.  Conversely, an example of an extreme {\emph
  exploration} strategy would be to randomly allocate the trials,
irrespective of the payoff rates you obtain.  Obviously, each of these
strategies is flawed.  What is an optimal strategy?
In \cite{Holland1973,Holland1975} Holland derived
an equation for such a strategy and argued that the optimal strategy
allocated trials to the observed best arm at a slightly higher rate than
exponential.  
% SF used 'argue' because some claim that his derivation was wrong.
He then extended this 
% Let $A$ denote the arm currently
%observed to have the higher payoff probability, and $B$ denote the
%other arm.  Holland showed that the optimal strategy (the one that
%yields maximal payoff, or minimal loss) is for the number of trials
%allocated to $A$ to grow slightly faster than an exponential function
%of the number of trials allocated to $B$.  Holland then showed that this
result to the {\emph multi-armed bandit} case.

In a population undergoing adaptation, building blocks consitute the arms of a multi-armed bandit.  Each
building block can be said to have a probability of ``payoff'' (i.e.,
contribution to a given individual's fitness).  Evaluating an
individual in an environment is like ``pulling the arms'' on a
multi-armed bandit, where the ``arms'' correspond to each of the
building blocks making up that individual.  Thus, in the process of
assigning a fitness to each individual in a population, adaptive
evolution can be seen as implicitly sampling the many building blocks
making up that collection of individuals.  The average fitness over
many individuals containing a particular building block gives an
estimate of that building block's payoff probability.

% The question of how to balance exploitation and exploration---how to
% optimally allocate trials to different arms based on their observed
% payoff---now becomes the question of how to optimally sample in the
% vast space of possible building blocks, based on their observed
% average fitness.  Of course adaptive evolution deals in populations of
% individuals, not building blocks.  There is no explicit mechanism for
% keeping statistics on building blocks' ``observed average fitness''.
% However, Holland's central idea here is that an approximation to
% optimal building-block sampling does indeed occur, as an emergent
% property of the population dynamics.

Holland argued that an approximation to
optimal building-block sampling actually occurs, as an emergent
property of the population dynamics defined by a
% To show this mathematically, he defined an idealization of
%adaptive evolution: an algorithm he called a {\emph reproductive
% plan}.
a population-based stochastic process
operating on bit strings, involving the ``genetic'' operators of
 reproduction, fitness-based selection, crossover, mutation, and
 inversion.  
%Building blocks are formalized as {\emph
%   schemata}---patterns of bits within a string. (Here are two examples
% of schemas: ``all strings beginning with the bits 10'' or ``all
% strings that start with the pattern 1*1'', where * can be replaced by
% either 0 or 1).

% Holland's most important result was the following: He proved
% mathematically that his reproductive plan results---implicitly---in a
% near-optimal allocation of trials to schemata, thus optimizing the
% exploitation versus exploration balance.  Holland noted that the
% explicit act of assigning fitnesses to the individuals in a population
% was actually implicitly sampling a much larger collection of building
% blocks.  Holland termed this {\emph implicit parallelism}; the
% parallel leveraging of statistics from implicit sampling is a main
% strength of his population-based approaches to search.

It was this formalization
% and resulting theorems that led to the
that led to the invention of genetic algorithms, which featured stochastic
population-based search and recombination as a critical
%{\emph crossover} as a critical
operation that allowed successful building blocks to be recombined and
tested in new contexts.  What made GAs unique
among other evolution-inspired algorithms at the time were the
mathematical foundations described above, the emphasis on populartions
and recombination as a central mechanism,
and a focus (at least in Holland's mind) on continual adaptation to
non-stationary environments rather than optimization to static
environments.

The framework Holland developed in \cite{Holland1975} was more general
than the genetic algorithm; its aim was an interdisciplinary theory of
adaptation, one that would inform biology, say, as much as computer
science \cite{Christiansen1998}.  The later, successful application of
genetic algorithms to real-world optimization and learning tasks was,
for Holland, just icing on the cake.

\section{Internal Models and Perpetual Novelty}

%One system of special interest to Holland was the human mind, and in
%particular, its capacity to learn via induction.

Internal models are central to Holland's theory of adaptive
systems.  He posits that all adaptive systems create and use internal
models to prosper in continually
changing environments---\emph{perpetual novelty}. Models can be tacit and learned
  through evolutionary time, as in the case of bacteria swimming up a
  chemical gradient, or explicit and learned over a single lifespan,
  as in the case of cognitive systems that incorporate experience into
  internal representations through learning.  In
  Holland's view, the key activity of an adaptive agent involves
  building and refining these data-driven models
  of the environment.
%using them to make predictions and take actions,
%  and occasionally receiving positive or negative feedbacks
%  (intermittent rewards).

In his second book, \emph{Induction} \cite{Holland1989}, Holland and
his co-authors tackled the question of how cognitive agents can
possibly learn and profit from
internal models, without supervision by combining
environmental inputs and rewards with stored knowledge.   In their
framework, a 
model defines a set of equivalence relations over environmental states,
together with a set of transition rules, which are learned over time
based on environmental rewards (or punishments).  Models that
form valid homomorphisms with the environment allow the system to make
accurate predictions.  In Holland's conception, the equivalence
classes are initially very general, say ``moving object'' and
``stationary object,'' and over time, through experience and learning,
 are specialized into more useful and precise subclasses, say
``insect'' and ``nest.''   Over time, the adaptive system
builds up a default hierarchy of rules covering general
cases and refinements for specific classes.  

% For example, a general rule might
% state that, ``If X is a bird, then X can fly'', whereas a more specific
% one might state, ``If X is a bird with small wings and a large body,
% then X cannot fly'' \cite{Holland1989}.  When faced with a bird in
% the environment, both rules might compete to post their conclusion,
% but the more specific one would be favored if both of its conditions are met.

% Maybe this should be moved to section on Complexity
Although the idea of default hierarchies was prevalent in 
knowledge representation systems of the era, Holland made two key
contributions.  The first was his emphasis on homomorphisms as a formal way to
evaluate model validity, and  idea that dates back to Ross Ashby's
\emph{An Introduction to Cybernetics} \cite{Ashby1956}. Holland's student,
Bernard Ziegler developed this idea into a formal theory of computer modeling
and simulation \cite{Ziegler1976}.  
% %Today, even though computational modeling is a major
% %application area of computing, essential to most natural sciences and
% %engineering disciplines
Even today, these early homomorphic theories of modeling
stand as the most elegant approach we know of to characterize when
a model is consistent with the environment and how an intelligent
agent, human or artificial, can update the model to better reflect
reality.

Holland's second key contribution was describing a computational
mechanism, the \emph{learning classifier
  system}~\cite{Holland1977,Holland1986}, to illustrate how a cognitive system could
iteratively build up a detailed and hierarchical model of its
environment to enhance survival.  The key learning elements of this method, the
bucket-brigade algorithm, combined with a genetic algorithm, presaged many of the ideas in modern reinforcement
learning, including unsupervised learning, non-Markovian learning, AND
ONE OTHER?.

% While genetic algorithms are nowadays most often associated with models of
% biological evolution, in Holland's view the processes of adaptation he
% was studying manifested themselves in many different complex systems.

% In the 1970s and 80s Holland formulated and extensively explored a
% particular model of decision-making, action, and inductive learning,
% called the {\emph classifier system} (e.g.,)
% .  In a classifier system, a population
% of ``if-then'' rules interacts with an environment, and over time is
% able to learn to improve its performance via both {\emph
%   credit-assignment} and {\emph rule-discovery} algorithms.

% Like genetic algorithms, classifier systems can be viewed both as 
% models of adaptation and as artificial-intelligence (AI) methods.  At
% the time Holland was developing classifier systems, the field of AI
% was focused on expert systems, which typically did not learn on their
% own, a fundamental deficiency in Holland's view: ``[Expert] systems
% are brittle in the sense that they respond appropriately only in
% narrow domains, requiring substantial human intervention to compensate
% for even slight shifts in domain'' \cite{Holland1986}. In
% contrast, Holland proposed that ``{\emph induction} is the basic, and
% perhaps only, way of making large advances in this direction.''

Holland's inspiration for classifier systems came from several
different disciplines, including Hebbian
learning, artificial intelligence, evolutionary biology, economics,
psychology, control theory, and other fields (e.g.,
\cite{Bellman1961,Samuel1959}).  Knowledge representation in the form of a
population of ``if-then'' rules seemed like a good choice, not only
because of its popularity in AI at the time but also from Holland's
early work on modeling Hebbian cell assemblies: ``In Hebbs view, a
cell assembly makes a simple statement: If such and such an event
occurs, then I will fire for a while at a high
rate.'' \cite{Waldrop1993} (p. 182).  
The if-then rules, when activated, compete to post their results on a
shared ``message list,'' modeling the system's short-term memory
(again inspired by Hebb's work and AI blackboard systems of the day).
Unlike the AI systems, however new rules were generated automatically,
in a trial-and-error fashion using a
genetic algorithm.  

Successful rules were
strengthened over time if their predictions led to positive rewards
from the environment 
(and weakened otherwise).  
through a credit-assignment method
called the {\emph bucket-brigade} algorithm, in which rules gaining
rewards from the environment or from other rules transferred some
of their gains to those earlier-firing ``stage-setting'' rules that
set up the conditions for the eventual reward.   [ONE SENTENCE HERE
ABOUT SAMUEL'S CHECKER PLAYING PROGRAM.]
%, e.g., finding a target, or avoiding a predator,

%Finally, new rules for the population are discovered by a genetic
%algorithm, using the strength of rules as a fitness function.
%Classifier systems are fairly complex architectures, with learning
%occurring at multiple time scales.  
Although Holland proposed classifier systems as an executable theory
of inductive processes in cognition, other researchers took the
computational system more seriously, applying it to areas as diverse
as poker-playing \cite{Smith1980}, control of gas pipeline
transmission \cite{Goldberg1983}, and modeling the stock market
\cite{Palmer1994}.  See Ref.~\cite{Booker1989} for more details about
practical applications of classifier systems.  Today, other
reinforcement learning methods are more popular for real-world
decision and control problems, but classifier systems can perhaps be
thought of as an essential ``stage-setting'' method that enabled the
development of later approaches.

% \footnote{Of course similar
%   credit-assignment methods have been used extensively in
%   reinforcement learning, neural networks, and other areas of modern
%   machine learning, but Holland was among the first to propose such
%   methods.}

Holland was primarily interested in how the two
learning mechanisms (discovery of new rules and apportioning credit to
existing rules) could work together to create 
%subsets of rules
%that accurately model the environment, and thus are able to ``fire''
%at appropriate times.  
%Holland envisioned that, with a complex environment and enough time
%for learning, a classifier system might learn an intricate default
useful default hierarchies of rules.  He emphasized that the competition inherent in the learning
and action mechanisms would allow the system to adapt to a continually
changing environment without losing what it had learned in the past.
Holland put it this way: ``Competition among rules provides the system
with a graceful way of handling perpetual novelty.  When a system has
strong rules that respond to a particular situation, that is the
equivalent of saying that it has certain well-validated hypotheses.
%Offspring rules, which begin life weaker than do their parents, can
%win the competition and influence the system's behavior only when
%there are not strong rules whose conditions are satisfied---in other
%words when the system does not know what to do.  If their actions
%help, they survive; if not they are soon replaced.  
%Thus,
... new rules do not interfere with the system's action in well-practiced
situations but wait gracefully in the wings as hypotheses about what
to do under novel circumstances." \cite{Holland1992}.  

\section{The Art of Scientific Modeling}

Given that Holland believed that the ability to learn and manipulate
internal models was essential for any adapting system, it is no
surprise that he viewed modeling as essential for scientific inquiry.

%Today, we use computational models in several distinct ways---as
%tools for analyzing data in statistical models; as methods for
%discovering new knowledge; and as a means to understand how complex
%systems works.  [Steph, I am not clear on the distinction between the
%second and third things here, so I combined them.  Change back if you
%want.]

Today, we use computational models both for \emph{prediction}---by
analyzing data via statistical models---and for \emph{understanding}
how systems work---by probing the effects of hypothesized underlying
mechanisms.  This latter use of models was dear to Holland's heart.
In his view, the key to science was understanding the mechanisms that
cause a system to behave in a certain way, an aspiration that goes
well beyond data fitting methods, which typically focus only on
describing the aggregate behavior of a system.  

For example, a statistical model that describes the boom and bust
pattern of the stock market cannot address the underlying mechanisms
that lead to these cycles, through the collective actions of myriad
individual buy/sell decisions.  Similarly, the genetic algorithms for
which Holland is so famous provide a simple computational framework in
which to explore the dynamics of Darwinian evolution and whether the
basic mechanisms of variation, differential reproduction, and heredity
are sufficient to account for the richness of our natural world.  

The emphasis on exploratory models to build intuitions was an
important theme of Holland's work, and he often quoted Eddington's
remark on the occasion of the first experimental test of Einstein's
theory of relativity: "The contemplation in natural science of a wider
domain than the actual leads to a far better understanding of the
actual" \cite{Eddington1927}.

Holland was interested in models that explored basic principles and
mechanisms, even if they didn't make specific or detailed predictions.
Such models can show generically how certain behaviors could be produced.
Holland pioneered a style of modeling that has come to be known as
'individual based' or 'agent based,' in which every component of a
system is represented explicitly and has state, e.g., every trader in
a stock market system or every cell in an immune system model.  In
such models, each agent has its own behavior rules, which it can
update (or learn) over time.  In order to capture the constraints of
systems living under spatial constraints, these models are often
defined over spatial structures, such as networks or simple grids.  

A given agent-based model encodes a theory about the mechanisms that
are relevant for producing the behavior of interest.  Similar to
expert systems, such models are especially useful for studying systems
that don't have analytic mathematical descriptions.  Agent-based
models can facilitate interdisciplinary collaborations because the
underlying rules can be easily communicated.  The agent-based models
championed by Holland were typically idealized versions of complex
systems and not intended to provide detailed, domain-specific
predictions.  Instead they were meant to explore possible general
mechanisms of complex systems and thus provide insights that might
lead to more specific, detailed models.  Such idealized models are
akin to what Dennett has called ``intuition pumps''
\cite{Dennett1984}.

It should be noted that Holland's view of modeling is by no means
typical.  For example, in a textbook on computational modeling, the
authors offer the following definition: “Modeling is the application
of methods to analyze complex, real-world problems in order to make
predictions about what might happen with various actions”
\cite{Shiflet2014}.   This sort of perspective completely rules out the
kind of exploratory modeling that Holland was most interested in. 

Some researchers dispute that models make any kind of scientific
contribution: "Models are metaphors that explain the world we don't
understand in terms of worlds we do.  They are merely analogies,
provide partial insight, stand on someone else's feet.  Theories stand
on their own feet, and rely on no analogies."  [Emanuel Derman, 2012].
[STEPH, I COULD NOT FIND REFERENCE FOR THIS QUOTE OUTSIDE SFI VIDEO.]

[NOW WE NEED A STRONG FINISHING SENTENCE TO RESCUE JHH STYLE
  MODELING.]

\section{Complexity}

%\section{Complex Adaptive Systems}

Holland was interested in a broad array of adaptive systems---immune systems, ecologies, financial markets, cities, and the brain---systems that are \emph{complex}.  In the 1980's, he teamed up with a small group of scientists, primarily physicists with a sprinkling of economists and biologists, to discuss what this wide swath of systems has in common.  The discussions helped define the intellectual mission of the Santa Fe Institute, the first institution dedicated to developing a science of complexity.  [add in how many complexity institutes there are worldwide and which others John played a hand in.]  Holland brought to these discussions his lifelong study of adaptation and a reminder that true theories about complexity would need to look deeper than phenomenological descriptions but also account for the  'how' and 'why' of these systems.
%describing phenomena like chaos, power laws, and ONE MORE, and acoount for the  
As the discussions matured, a consensus developed about the basic elements of complexity---systems that: (1)  are composed of many components with nontrivial (nonlinear) interactions; (2) behavior that emerges from the dynamics of the system, exhibiting higher-order patterns; (3) Scale, systems are nested and structure/behavior emerges at different scales, with some behavior being conserved across all scales and other behaviors changing at different scales, and (4) continual adaptation where systems adjust their behavioral rules through evolution and learning.  Although this is far from a formal definition of complex systems, most people working in the field today are interested in systems that have these properties.

To illustrate his ideas about the ubuiquity of adaptation, Holland developed two systems, ECHO and the SFI stock market.
Although less famous than genetic algorithms and learning classifier systems, ECHO was no less ambitious.  It's scope included the evolution of interacting populations of individual agents, which through competition and learning, discover symbiotic triads such as the famous ant-fly-caterpillar interaction,  the Wiksell triangle of trading relationships in economics, and the immune system's learned ability to distinguish 'self' from 'other.'  Without going into the details of the model [CITES HERE], ECHO formalized Holland's theories about complex adapting systems into a runnable computational system where agents evolved external markers (called tags) and internal preferences and used them to form higher level aggregate structures (trading relationships, symbiotic groups, trophic cascades, interdependent organizations etc.).  ECHO agents sometimes discovered mimicry to deceive competitors, and over time, the model developed increasingly complex structures and behaviors, often reproducing patterns observed in nature.  For example,  ECHO evolved a diversity of agent types, whose rank-frequency distribution closely parallels the well-known Preston curve in ecology, a quantiative statement of the adage that 'most species are rare'. Many of the insights behind this project are described in his book, [WHICH ONES]. The broad scope of this project was appealing to immunologists, economists, and evolutionary biologists alike, but it was the economists who took it one step further in the SFI stock market.

% Note: SF has the history backwards here.  We should correct that, but the story was so nice in this order

In the early 1990s, Holland teamed up with other SFI researchers, including two economists, to tackle the mismatch between predictions of the dominant theory in economics, the rational expectations hypothesis, and empirically observed stock market behaviors that deviated from the predicted equilibria solutions.  In brief, most economic theory of the day assumed that all participants in an economy or financial market are 100\% rational and act to maximize their individual gain.  We know, however, that real actors in real economies and markets are rarely rational, and financial markets often deviate from rationalilty, for example with speculative bubbles and crashes.  The SFI stock market project studied what happens in a model where rational traders are replaced by traders who learn to forecast stock prices over time.  The model was constructed to allow testing for the possible emergence of fundamental trading versus technical trading versus uninformed trading. The simulated market with (learning) trading agents was run many times and the dynamics of price and trading volumes were compared to observed patterns in real markets, replicating many of the features of real-life markets.  Although the model was primitive in many ways, it was wildly influential and led to many follow-on projects  It also demonstrated the important role that adaptation plays in complex systems and illustrated how Holland's theories of continual learning in response to intermittent feedbacks from the environment could be integrated into domain-specific settings.

%Blake LeBaron (economics);
% W. Brian Arthur (economics); 
% John Holland (psychology/EE/CS,
% and father of GAs);
% Richard Palmer (physics);
% Paul Taylor (computer science

%Many modeling issues not satisfactorily resolved by the SF-ASM model have been 
% taken up in later research (see Ref.[4]).

Holland's later books,  \emph{Emergence}, \emph{Signals and Boundaries}, and \emph{Complexity, A Summary} show how the theories of adaptation that he developed during the earlier part of his career fit into the larger landscape of complex systems research.   Holland's focus on how complex patterns emerge and change, rather than simply characterizing the patterns themselves (e.g., describing chaotic attractors or power laws) reflected his deterimination to 'get to the heart' of complex systems.  This determination 
represents the best of science.  Holland's willingness to tackle the hardest questions, develop his own formalisms, and use mathematics productively sets a high bar that we all should aspire to.


\section{Relevance to Modern CS}

\begin{itemize}
\item Evolutionary computation
\item Q Learning
\item Backprop
\item    From Turing nomination: "Holland's machine learning system known as the
   Learning Classifier System (LCS), developed in the early 1980s,
   incorporated a reinforcement learning algorithm known as the bucket
   brigade for non-Markovian environments, anticipating by nearly a
   decade non-Markovian learning algorithms."
\item Exploitation versus exploration -- relevance to reinforcement
  learning, other parts of machine learning and optimization.
\item Active learning.  (Two-armed bandit problem.) 
\item On-line learning.  
\end{itemize}

\section{Conclusion}

Introduced a few generations of students to computation in natural systems, an idea that today is better accepted.  His insights were deeper and more general than what often passes for work in biomimicry, e.g., for robots.

The ideas have had huge impact and should still be a beacon for research in intelligent and complex systems

John's personality and humanity is inextricably tangled up with his intellectual contributions.

\bibliographystyle{plain}

\bibliography{paper}

\end{document}
